#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Ç–µ—Å—Ç–æ–≤
"""

import os
import shutil
import json
from pathlib import Path
from collections import defaultdict

def analyze_postman_collection():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Postman –∫–æ–ª–ª–µ–∫—Ü–∏—é –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫—Ä—ã—Ç—ã—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤"""
    backend_dir = Path(__file__).parent.parent
    collection_file = backend_dir / "AutoRia_API_Final_Collection.postman_collection.json"
    
    if not collection_file.exists():
        print("‚ùå Postman –∫–æ–ª–ª–µ–∫—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return set()
    
    try:
        with open(collection_file, 'r', encoding='utf-8') as f:
            collection = json.load(f)
        
        endpoints = set()
        
        def extract_endpoints(item):
            if 'request' in item:
                if 'url' in item['request'] and 'raw' in item['request']['url']:
                    url = item['request']['url']['raw']
                    method = item['request'].get('method', 'GET')
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –±–µ–∑ –±–∞–∑–æ–≤–æ–≥–æ URL
                    if '{{base_url}}' in url:
                        path = url.replace('{{base_url}}', '').strip('/')
                        endpoints.add(f"{method} /{path}")
            
            if 'item' in item:
                for subitem in item['item']:
                    extract_endpoints(subitem)
        
        if 'item' in collection:
            for item in collection['item']:
                extract_endpoints(item)
        
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(endpoints)} —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ –≤ Postman –∫–æ–ª–ª–µ–∫—Ü–∏–∏")
        return endpoints
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
        return set()

def analyze_test_files():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã"""
    backend_dir = Path(__file__).parent.parent
    
    test_dirs = [
        backend_dir / "tests",
        backend_dir / "tests_complete_184", 
        backend_dir / "tests_integration"
    ]
    
    test_analysis = {}
    
    for test_dir in test_dirs:
        if not test_dir.exists():
            continue
            
        dir_name = test_dir.name
        test_analysis[dir_name] = {
            'files': [],
            'total_size': 0,
            'endpoints_covered': set()
        }
        
        for test_file in test_dir.glob("**/*.py"):
            if test_file.name.startswith('test_'):
                file_size = test_file.stat().st_size
                test_analysis[dir_name]['files'].append({
                    'name': test_file.name,
                    'path': test_file,
                    'size': file_size,
                    'size_kb': file_size // 1024
                })
                test_analysis[dir_name]['total_size'] += file_size
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
                try:
                    with open(test_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è API –ø—É—Ç–µ–π
                        import re
                        api_patterns = re.findall(r'["\']/(api/[^"\']*)["\']', content)
                        for pattern in api_patterns:
                            test_analysis[dir_name]['endpoints_covered'].add(pattern)
                except:
                    pass
    
    return test_analysis

def identify_duplicates_and_obsolete(test_analysis, postman_endpoints):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ç–µ—Å—Ç—ã"""
    
    print("\nüîç –ê–ù–ê–õ–ò–ó –¢–ï–°–¢–û–í–´–• –§–ê–ô–õ–û–í")
    print("=" * 60)
    
    recommendations = {
        'keep': [],
        'archive': [],
        'delete': []
    }
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –ø–∞–ø–∫—É
    for dir_name, data in test_analysis.items():
        print(f"\nüìÅ {dir_name}:")
        print(f"  üìä –§–∞–π–ª–æ–≤: {len(data['files'])}")
        print(f"  üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä: {data['total_size'] // 1024}KB")
        print(f"  üìä –ü–æ–∫—Ä—ã—Ç—ã—Ö —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤: {len(data['endpoints_covered'])}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
        for file_info in sorted(data['files'], key=lambda x: x['size'], reverse=True):
            print(f"    üìÑ {file_info['name']} ({file_info['size_kb']}KB)")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ—á–∏—Å—Ç–∫–µ
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    
    # tests/ - –ø—Ä–æ—Å—Ç–∞—è –ø–∞–ø–∫–∞ —Å –æ–¥–Ω–∏–º —Ñ–∞–π–ª–æ–º
    if 'tests' in test_analysis:
        files = test_analysis['tests']['files']
        if len(files) == 1 and files[0]['size_kb'] < 50:
            recommendations['delete'].extend([f['path'] for f in files])
            print(f"  üóëÔ∏è tests/ - —É–¥–∞–ª–∏—Ç—å (–¥—É–±–ª–∏—Ä—É–µ—Ç Postman)")
    
    # tests_complete_184/ - –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ç–µ—Å—Ç—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    if 'tests_complete_184' in test_analysis:
        files = test_analysis['tests_complete_184']['files']
        large_files = [f for f in files if f['size_kb'] > 20]
        small_files = [f for f in files if f['size_kb'] <= 20]
        
        if large_files:
            recommendations['keep'].extend([f['path'] for f in large_files])
            print(f"  ‚úÖ tests_complete_184/ - –æ—Å—Ç–∞–≤–∏—Ç—å –∫—Ä—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã ({len(large_files)} —à—Ç)")
        
        if small_files:
            recommendations['archive'].extend([f['path'] for f in small_files])
            print(f"  üìÅ tests_complete_184/ - –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å –º–µ–ª–∫–∏–µ —Ñ–∞–π–ª—ã ({len(small_files)} —à—Ç)")
    
    # tests_integration/ - –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
    if 'tests_integration' in test_analysis:
        files = test_analysis['tests_integration']['files']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã
        important_keywords = ['comprehensive', 'final', 'full_system', 'workflow']
        important_files = []
        other_files = []
        
        for f in files:
            if any(keyword in f['name'].lower() for keyword in important_keywords):
                important_files.append(f)
            else:
                other_files.append(f)
        
        if important_files:
            recommendations['keep'].extend([f['path'] for f in important_files])
            print(f"  ‚úÖ tests_integration/ - –æ—Å—Ç–∞–≤–∏—Ç—å –≤–∞–∂–Ω—ã–µ —Ç–µ—Å—Ç—ã ({len(important_files)} —à—Ç)")
        
        if other_files:
            recommendations['archive'].extend([f['path'] for f in other_files])
            print(f"  üìÅ tests_integration/ - –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ ({len(other_files)} —à—Ç)")
    
    return recommendations

def execute_cleanup(recommendations):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ—á–∏—Å—Ç–∫—É —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º"""
    backend_dir = Path(__file__).parent.parent
    
    # –°–æ–∑–¥–∞–µ–º –∞—Ä—Ö–∏–≤–Ω—É—é –ø–∞–ø–∫—É
    archive_dir = backend_dir / "archive" / "old_tests"
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nüßπ –í–´–ü–û–õ–ù–ï–ù–ò–ï –û–ß–ò–°–¢–ö–ò")
    print("=" * 40)
    
    # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
    deleted_count = 0
    for file_path in recommendations['delete']:
        if file_path.exists():
            file_path.unlink()
            print(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω: {file_path.name}")
            deleted_count += 1
    
    # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
    archived_count = 0
    for file_path in recommendations['archive']:
        if file_path.exists():
            # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–ø–∞–ø–∫—É –≤ –∞—Ä—Ö–∏–≤–µ –ø–æ –∏–º–µ–Ω–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –ø–∞–ø–∫–∏
            parent_name = file_path.parent.name
            archive_subdir = archive_dir / parent_name
            archive_subdir.mkdir(exist_ok=True)
            
            archive_path = archive_subdir / file_path.name
            shutil.move(str(file_path), str(archive_path))
            print(f"  üìÅ –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω: {file_path.name} ‚Üí archive/old_tests/{parent_name}/")
            archived_count += 1
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    kept_count = len(recommendations['keep'])
    print(f"  ‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–∞–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {kept_count}")
    
    return deleted_count, archived_count, kept_count

def cleanup_empty_dirs():
    """–£–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏"""
    backend_dir = Path(__file__).parent.parent
    
    test_dirs = [
        backend_dir / "tests",
        backend_dir / "tests_complete_184",
        backend_dir / "tests_integration"
    ]
    
    removed_dirs = 0
    for test_dir in test_dirs:
        if test_dir.exists():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ Python —Ñ–∞–π–ª—ã
            python_files = list(test_dir.glob("**/*.py"))
            if not python_files or (len(python_files) == 1 and python_files[0].name == '__init__.py'):
                # –ü–∞–ø–∫–∞ –ø—É—Å—Ç–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ __init__.py
                shutil.rmtree(test_dir)
                print(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞: {test_dir.name}")
                removed_dirs += 1
    
    return removed_dirs

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üßπ –û–ß–ò–°–¢–ö–ê –î–£–ë–õ–ò–†–£–Æ–©–ò–•–°–Ø –¢–ï–°–¢–û–í")
    print("=" * 60)
    
    # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º Postman –∫–æ–ª–ª–µ–∫—Ü–∏—é
    postman_endpoints = analyze_postman_collection()
    
    # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
    test_analysis = analyze_test_files()
    
    # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ —É–¥–∞–ª–∏—Ç—å/–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å
    recommendations = identify_duplicates_and_obsolete(test_analysis, postman_endpoints)
    
    # 4. –í—ã–ø–æ–ª–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É
    deleted, archived, kept = execute_cleanup(recommendations)
    
    # 5. –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏
    removed_dirs = cleanup_empty_dirs()
    
    print(f"\n" + "=" * 60)
    print("üéâ –û–ß–ò–°–¢–ö–ê –¢–ï–°–¢–û–í –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {deleted}")
    print(f"  üìÅ –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {archived}")
    print(f"  ‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω–æ –≤–∞–∂–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {kept}")
    print(f"  üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫: {removed_dirs}")
    print(f"\nüí° Postman –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ 177 —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤")
    print(f"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏ –≤–∞–∂–Ω—ã–µ —Ç–µ—Å—Ç—ã")

if __name__ == "__main__":
    main()
